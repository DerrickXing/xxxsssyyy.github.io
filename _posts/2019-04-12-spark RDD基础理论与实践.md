---
layout:     post                    # 使用的布局（不需要改）
title:      spark RDD基础理论与实践    # 标题 
subtitle:   弹性分布式数据集           #副标题
date:       2019-04-12              # 时间
author:     Derrick                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - spark 
    - 大数据
---
# 前言
  最近学习了大规模数据处理框架，现阶段主要用到的是spark的批数据处理。这篇博客主要对spark的核心RDD
  进行总结[spark2.4官方文档](http://spark.apache.org/docs/latest)中关于RDD(resilient distributed dataset)
  进行应用层面的总结。
## 背景
  Spark最初由美国加州伯克利大学(UCBerkeley)的AMP实验室于 2009年开发，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程   序,其论文原文地址为[《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf)。所谓的RDD（弹性分布式数据集）是该实验室所取的一个高大上抽象的名字，虽说如此，但这并不妨碍其受欢迎程度以及实用性。渣渣我后续一定认真拜读论文。Spark在2014年打破了Hadoop保持的基准排序纪录，Spark用十分之一的计算资源，获得了比Hadoop快3倍的速度，从此受到相关企业以及研究人员的广泛关注。
## Spark开发动机
  Spark开发的首要动机是以Hadoop为代表的很多分布式计算框架无法实现高效的迭代式计算以及交互式数据挖掘。相比于Hadoop MapReduce，Spark主要具有如下优点:（1）Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还 提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活。（2）Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高。Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制。
# 正文
  Spark的RDD支持两种类型的操作
  
  动作(action):在数据集上进行运算，返回计算值
  
  转换(transformation): 基于现有的数据集创建一个新的数据集
