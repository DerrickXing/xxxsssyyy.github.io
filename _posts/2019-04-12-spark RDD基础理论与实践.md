---
layout:     post                    # 使用的布局（不需要改）
title:      spark RDD基础理论与实践    # 标题 
subtitle:   弹性分布式数据集           #副标题
date:       2019-04-12              # 时间
author:     Derrick                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - spark 
    - 大数据
---
# 前言
  最近学习了大规模数据处理框架，现阶段主要用到的是spark的批数据处理。这篇博客主要对spark的核心RDD
  进行总结[spark2.4官方文档](http://spark.apache.org/docs/latest)中关于RDD(resilient distributed dataset)
  进行应用层面的总结。
## 背景
  Spark最初由美国加州伯克利大学(UCBerkeley)的AMP实验室于 2009年开发，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程   序,其论文原文地址为[《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf)。所谓的RDD（弹性分布式数据集）是该实验室所取的一个高大上抽象的名字，虽说如此，但这并不妨碍其受欢迎程度以及实用性。渣渣我后续一定认真拜读论文。Spark在2014年打破了Hadoop保持的基准排序纪录，Spark用十分之一的计算资源，获得了比Hadoop快3倍的速度，从此受到相关企业以及研究人员的广泛关注。
## Spark开发动机
  Spark开发的首要动机是以Hadoop为代表的很多分布式计算框架无法实现高效的迭代式计算以及交互式数据挖掘。相比于Hadoop MapReduce，Spark主要具有如下优点:（1）Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还 提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活。（2）Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高。Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制。
# 正文
## 初始化Spark
  Spark程序必须做的第一件事是创建一个SparkContext对象，它告诉Spark如何访问集群。要创建SparkContext首先需要构建一个包含有关应用程序信息的SparkConf对象。
  ```
  val conf = new SparkConf().setAppName(appName).setMaster(master)
  new SparkContext(conf)
  ```
  该appName参数是应用程序在集群UI上显示的名称。 master是Spark，Mesos或YARN群集URL，或以本地模式运行的特殊“本地”字符串。实际上，当在群集上运行时，您不希望master在程序中进行硬编码，而是启动应用程序spark-submit并在那里接收它。但是，对于本地测试和单元测试，您可以传递“local”以在进程中运行Spark。
## Spark操作
  Spark的RDD支持两种类型的操作
### 转换(transformation): 基于现有的数据集创建一个新的数据集
  `map(func)`:返回通过函数func传递源的每个元素形成的新分布式数据集。
  
  `filter(func)`:返回通过选择func返回true的源元素形成的新数据集。
  
  `flatMap(func)`:与map类似，但每个输入项可以映射到0个或更多输出项（因此func应该返回Seq而不是单个项）。
  实例1：
  ```
  val rdd = sc.parallelize(Seq("Roses are red", "Violets are blue"))  // lines
  rdd.collect
  ```
  ```
  res0: Array[String] = Array("Roses are red", "Violets are blue")
  ```
  ```
  rdd.flatMap(_.split(" ")).collect	
  ```
  ```
  res2: Array[String] = Array（"Roses", "are", "red", "Violets", "are", "blue"）
  ```
  示例2：
  ```
  myRDD.flatMap(x =>new Seq(2*x,3*x))
  ```
  
  `filter(func)`:返回通过选择func返回true的源元素形成的新数据集。
  
  `distinct([numPartitions])`:返回包含源数据集的不同元素的新数据集。
  
  `groupByKey([numPartitions])`:在（K，V）对的数据集上调用时，返回（K，Iterable <V>）对的数据集。
   注意：如果要进行分组以便对每个密钥执行聚合（例如总和或平均值），则使用reduceByKey或aggregateByKey将产生更好的性能。 
   注意：默认情况下，输出中的并行级别取决于父RDD的分区数。您可以传递可选numPartitions参数来设置不同数量的任务。
    
### 动作(action):在数据集上进行运算，返回计算值
  `reduce(func)`:使用函数func（它接受两个参数并返回一个）来聚合数据集的元素。该函数应该是可交换的和关联的，以便可以并行正确计算。
  
  `collect()`:在驱动程序中将数据集的所有元素作为数组返回。在过滤器或其他返回足够小的数据子集的操作之后，这通常很有用。
  
  `count()`:返回数据集中的元素数。
  
  `take(n)`:返回包含数据集的前n个元素的数组。
  
  `first()`:返回数据集的第一个元素（类似于take(1)）
  
  `takeOrdered(n, [ordering])`:使用自然顺序或自定义比较器返回RDD 的前n个元素
  
  `saveAsTextFile(path)`:将数据集的元素作为文本文件（或文本文件集）写入本地文件系统，HDFS或任何其他Hadoop
					 支持的文件系统的给定目录中。Spark将在每个元素上调用toString，将其转换为文件中的一行文本。                    
